{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mouni\\AppData\\Local\\Temp\\ipykernel_11072\\4099455718.py:647: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "INFO:     Started server process [11072]\n",
      "INFO:     Waiting for application startup.\n",
      "2025-03-29 21:59:03,202 - __main__ - INFO - Current query IDs in cache: []\n",
      "2025-03-29 21:59:03,203 - __main__ - INFO - Loading model: google/flan-t5-large\n",
      "Device set to use cpu\n",
      "2025-03-29 21:59:04,937 - __main__ - INFO - Loading embedding model...\n",
      "2025-03-29 21:59:04,949 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-03-29 21:59:07,688 - asyncio - ERROR - Task exception was never retrieved\n",
      "future: <Task finished name='Task-20' coro=<Server.serve() done, defined at c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\uvicorn\\server.py:68> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mouni\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mouni\\AppData\\Local\\Temp\\ipykernel_11072\\2946587643.py\", line 876, in <module>\n",
      "    server.run()\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\uvicorn\\server.py\", line 66, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\asyncio\\tasks.py\", line 339, in __wakeup\n",
      "    self.__step()\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\asyncio\\tasks.py\", line 256, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\uvicorn\\server.py\", line 70, in serve\n",
      "    await self._serve(sockets)\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\contextlib.py\", line 126, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\uvicorn\\server.py\", line 330, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n",
      "2025-03-29 21:59:11,682 - __main__ - INFO - Models loaded successfully\n",
      "2025-03-29 21:59:11,683 - __main__ - INFO - Loading and processing documents...\n",
      "2025-03-29 21:59:23,522 - __main__ - INFO - Processed 460 chunks for criminal\n",
      "2025-03-29 21:59:23,526 - __main__ - INFO - Extracted 4 articles from criminal\n",
      "2025-03-29 21:59:31,352 - __main__ - INFO - Processed 252 chunks for constitution\n",
      "2025-03-29 21:59:31,360 - __main__ - INFO - Extracted 148 articles from constitution\n",
      "2025-03-29 21:59:31,363 - __main__ - INFO - Documents loaded. Available fields: ['criminal', 'constitution']\n",
      "2025-03-29 21:59:31,365 - __main__ - INFO - API startup completed\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52207 - \"GET /stats HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52211 - \"GET /stats HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52230 - \"GET /stats HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 22:00:33,369 - __main__ - INFO - Using cached documents\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52234 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52376 - \"GET /stats HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52378 - \"GET /stats HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52382 - \"GET /stats HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 22:04:39,135 - __main__ - INFO - Using cached documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52386 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52404 - \"GET /stats HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52426 - \"GET /stats HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 22:07:53,571 - __main__ - INFO - Using cached documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52428 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52484 - \"GET /stats HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from functools import lru_cache\n",
    "from typing import List, Dict, Optional, Set, Tuple\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks, status\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uvicorn\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"legal_rag.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Tunisian Legal RAG API\",\n",
    "    description=\"A Retrieval-Augmented Generation system for Tunisian legal documents\",\n",
    "    version=\"2.0.0\"\n",
    ")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# ================= Configuration =================\n",
    "class ModelType(str, Enum):\n",
    "    FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
    "    DEEPSEEK_LLM = \"deepseek-ai/deepseek-llm-7b\"\n",
    "    MISTRAL = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"model_id\": \"google/flan-t5-large\",\n",
    "    \"cache_dir\": \"./model_cache\",\n",
    "    \"quantize\": False,\n",
    "    \"use_gpu\": True if torch.cuda.is_available() else False,\n",
    "    \"max_length\": 512,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95\n",
    "}\n",
    "\n",
    "DOC_CONFIG = {\n",
    "    \"criminal\": {\n",
    "        \"path\": \"./Tunisia_Crim-Fr.pdf\",\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 100\n",
    "    },\n",
    "    \"constitution\": {\n",
    "        \"path\": \"./Constitution_fr.pdf\",\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 100\n",
    "    }\n",
    "}\n",
    "\n",
    "LEGAL_TERMS = {\n",
    "    \"imprescriptibilité\": [\"prescription\", \"délai\", \"expiration\", \"crime\", \"poursuites\"],\n",
    "    \"droit\": [\"liberté\", \"garantie\", \"protection\"],\n",
    "    \"justice\": [\"tribunal\", \"magistrat\", \"judiciaire\"],\n",
    "}\n",
    "\n",
    "# ================= Data Models =================\n",
    "class FeedbackType(str, Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEGATIVE = \"negative\"\n",
    "    CORRECTION = \"correction\"\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str = Field(..., description=\"The legal question to ask\")\n",
    "    field: str = Field(..., description=\"Legal domain (criminal/constitution)\")\n",
    "    language: str = Field(\"fr\", description=\"Response language (fr/en)\")\n",
    "    top_k: int = Field(3, description=\"Number of documents to retrieve\", ge=1, le=5)\n",
    "    max_tokens: int = Field(150, description=\"Max tokens to generate\", ge=50, le=500)\n",
    "    temperature: float = Field(0.7, description=\"Generation temperature\", ge=0.1, le=1.0)\n",
    "    enable_reflection: bool = Field(True, description=\"Enable self-reflection\")\n",
    "\n",
    "class RetrievedDocument(BaseModel):\n",
    "    text: str\n",
    "    score: float\n",
    "    metadata: Dict\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    answer: str\n",
    "    retrieved_documents: List[RetrievedDocument]\n",
    "    query_time_ms: float\n",
    "    query_id: str\n",
    "    reflection: Optional[str] = None\n",
    "    cache_hit: bool = False\n",
    "\n",
    "class FeedbackRequest(BaseModel):\n",
    "    query_id: str = Field(..., description=\"ID of the query being rated\")\n",
    "    feedback_type: FeedbackType = Field(..., description=\"Type of feedback\")\n",
    "    correction_text: Optional[str] = Field(None, description=\"Corrected answer if type is correction\")\n",
    "    comments: Optional[str] = Field(None, description=\"Additional feedback comments\")\n",
    "\n",
    "class ReflectionRequest(BaseModel):\n",
    "    query_id: str = Field(..., description=\"ID of query to reflect on\")\n",
    "    reflection_prompt: Optional[str] = Field(\n",
    "        default=\"Analyze the quality of this response and suggest improvements:\",\n",
    "        description=\"Optional custom reflection prompt\"\n",
    "    )\n",
    "\n",
    "# ================= Core Components =================\n",
    "class DocumentCache:\n",
    "    def __init__(self):\n",
    "        self.texts = {}\n",
    "        self.bm25 = {}\n",
    "        self.embeddings = {}\n",
    "        self.articles = {}\n",
    "        self.metadata = {}\n",
    "        self.last_loaded = None\n",
    "        self.responses = {}\n",
    "        self.feedback = {}\n",
    "        self.query_metadata = {}\n",
    "\n",
    "cache = DocumentCache()\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipeline = None\n",
    "        self.embedding_model = None\n",
    "        self.is_loaded = False\n",
    "\n",
    "    async def load_models(self):\n",
    "        logger.info(f\"Loading model: {self.config['model_id']}\")\n",
    "        \n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config[\"model_id\"],\n",
    "                cache_dir=self.config[\"cache_dir\"]\n",
    "            )\n",
    "\n",
    "            # Load model with appropriate class\n",
    "            if \"flan-t5\" in self.config[\"model_id\"].lower():\n",
    "                model_class = AutoModelForSeq2SeqLM\n",
    "            else:\n",
    "                model_class = AutoModelForCausalLM\n",
    "\n",
    "            self.model = model_class.from_pretrained(\n",
    "                self.config[\"model_id\"],\n",
    "                cache_dir=self.config[\"cache_dir\"]\n",
    "            )\n",
    "\n",
    "            # Quantization\n",
    "            if self.config[\"quantize\"]:\n",
    "                logger.info(\"Applying quantization...\")\n",
    "                self.model = torch.quantization.quantize_dynamic(\n",
    "                    self.model,\n",
    "                    {torch.nn.Linear},\n",
    "                    dtype=torch.float16\n",
    "                )\n",
    "\n",
    "            # GPU support\n",
    "            if torch.cuda.is_available():\n",
    "                logger.info(\"Moving model to GPU...\")\n",
    "                self.model.to('cuda')\n",
    "\n",
    "            # Create pipeline\n",
    "            device = 0 if torch.cuda.is_available() else -1\n",
    "            self.pipeline = pipeline(\n",
    "                task=\"text2text-generation\" if \"flan-t5\" in self.config[\"model_id\"].lower() else \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            # Load embedding model\n",
    "            logger.info(\"Loading embedding model...\")\n",
    "            self.embedding_model = SentenceTransformer(\n",
    "                'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            )\n",
    "\n",
    "            self.is_loaded = True\n",
    "            logger.info(\"Models loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model loading failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @lru_cache(maxsize=1000)\n",
    "    def generate_response(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Generate response with caching\"\"\"\n",
    "        if not self.is_loaded:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "\n",
    "        # Default generation parameters\n",
    "        generation_params = {\n",
    "            \"max_new_tokens\": kwargs.get(\"max_tokens\", self.config[\"max_length\"]),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.config[\"temperature\"]),\n",
    "            \"top_p\": kwargs.get(\"top_p\", self.config[\"top_p\"]),\n",
    "            \"do_sample\": True,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"num_beams\": 5,\n",
    "            \"early_stopping\": True\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = self.pipeline(prompt, **generation_params)\n",
    "            \n",
    "            if isinstance(response, list):\n",
    "                generated_text = response[0][\"generated_text\"]\n",
    "            else:\n",
    "                generated_text = response\n",
    "            \n",
    "            # Remove prompt if it's included in response\n",
    "            if generated_text.startswith(prompt):\n",
    "                generated_text = generated_text[len(prompt):].strip()\n",
    "                \n",
    "            return generated_text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Generation failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for texts\"\"\"\n",
    "        if not self.embedding_model:\n",
    "            raise ValueError(\"Embedding model not loaded\")\n",
    "\n",
    "        try:\n",
    "            embeddings = self.embedding_model.encode(\n",
    "                texts,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "            return embeddings.cpu().numpy()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Embedding generation failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class DocumentManager:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    async def load_documents(self, force_reload: bool = False) -> bool:\n",
    "        \"\"\"Load and process all documents\"\"\"\n",
    "        current_time = datetime.now()\n",
    "\n",
    "        # Skip if recently loaded\n",
    "        if not force_reload and cache.last_loaded and \\\n",
    "           (current_time - cache.last_loaded).total_seconds() < 3600 and \\\n",
    "           cache.texts:\n",
    "            logger.info(\"Using cached documents\")\n",
    "            return True\n",
    "\n",
    "        logger.info(\"Loading and processing documents...\")\n",
    "        success = False\n",
    "\n",
    "        for field, field_config in self.config.items():\n",
    "            pdf_path = field_config[\"path\"]\n",
    "            \n",
    "            if not os.path.exists(pdf_path):\n",
    "                logger.error(f\"Document not found: {pdf_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                await self._process_document(\n",
    "                    pdf_path,\n",
    "                    field,\n",
    "                    field_config[\"chunk_size\"],\n",
    "                    field_config[\"chunk_overlap\"]\n",
    "                )\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {field}: {str(e)}\")\n",
    "\n",
    "        if success:\n",
    "            cache.last_loaded = current_time\n",
    "            logger.info(f\"Documents loaded. Available fields: {list(cache.texts.keys())}\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    async def _process_document(self, pdf_path: str, field: str, \n",
    "                              chunk_size: int, chunk_overlap: int):\n",
    "        \"\"\"Process a single document\"\"\"\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pages = loader.load()\n",
    "\n",
    "        # Process in parallel\n",
    "        await asyncio.gather(\n",
    "            self._process_regular_chunks(pages, field, chunk_size, chunk_overlap),\n",
    "            self._process_article_chunks(pages, field)\n",
    "        )\n",
    "\n",
    "    async def _process_regular_chunks(self, pages: List, field: str,\n",
    "                                    chunk_size: int, chunk_overlap: int):\n",
    "        \"\"\"Process document into searchable chunks\"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        documents = text_splitter.split_documents(pages)\n",
    "\n",
    "        # Extract content and metadata\n",
    "        texts = []\n",
    "        metadata = []\n",
    "        for doc in documents:\n",
    "            texts.append(doc.page_content)\n",
    "            metadata.append({\n",
    "                \"page\": doc.metadata.get(\"page\", 0),\n",
    "                \"source\": os.path.basename(doc.metadata.get(\"source\", \"\")),\n",
    "                \"id\": f\"{field}_{doc.metadata.get('page', 0)}_{len(texts)}\"\n",
    "            })\n",
    "\n",
    "        # Store in cache\n",
    "        cache.texts[field] = texts\n",
    "        cache.metadata[field] = metadata\n",
    "\n",
    "        # Create BM25 index\n",
    "        tokenized_texts = [text.split() for text in texts]\n",
    "        cache.bm25[field] = BM25Okapi(tokenized_texts)\n",
    "\n",
    "        # Generate embeddings\n",
    "        cache.embeddings[field] = await model_manager.get_embeddings(texts)\n",
    "\n",
    "        logger.info(f\"Processed {len(texts)} chunks for {field}\")\n",
    "\n",
    "    async def _process_article_chunks(self, pages: List, field: str):\n",
    "        \"\"\"Extract articles from document\"\"\"\n",
    "        article_pattern = re.compile(r'Article\\s+(\\d+)\\s*[:.]', re.IGNORECASE)\n",
    "        cache.articles.setdefault(field, {})\n",
    "\n",
    "        for page in pages:\n",
    "            content = page.page_content\n",
    "            article_matches = list(article_pattern.finditer(content))\n",
    "\n",
    "            for i, match in enumerate(article_matches):\n",
    "                article_num = match.group(1)\n",
    "                start_pos = match.start()\n",
    "                end_pos = article_matches[i+1].start() if i < len(article_matches)-1 else len(content)\n",
    "                article_text = content[start_pos:end_pos].strip()\n",
    "\n",
    "                cache.articles[field][article_num] = {\n",
    "                    \"text\": article_text,\n",
    "                    \"metadata\": {\n",
    "                        \"page\": page.metadata.get(\"page\", 0),\n",
    "                        \"source\": os.path.basename(page.metadata.get(\"source\", \"\")),\n",
    "                        \"id\": f\"{field}_article_{article_num}\",\n",
    "                        \"article\": int(article_num)\n",
    "                    }\n",
    "                }\n",
    "\n",
    "        logger.info(f\"Extracted {len(cache.articles[field])} articles from {field}\")\n",
    "\n",
    "class QueryProcessor:\n",
    "    def __init__(self, model_manager: ModelManager):\n",
    "        self.model_manager = model_manager\n",
    "\n",
    "    async def preprocess_query(self, query: str) -> Dict:\n",
    "        \"\"\"Analyze and extract key information from query\"\"\"\n",
    "        # Extract article number if specified\n",
    "        article_match = re.search(r'article\\s+(\\d+)', query, re.IGNORECASE)\n",
    "        \n",
    "        # Extract mentioned legal concepts\n",
    "        query_lower = query.lower()\n",
    "        mentioned_concepts = []\n",
    "        for concept, related_terms in LEGAL_TERMS.items():\n",
    "            if concept.lower() in query_lower:\n",
    "                mentioned_concepts.append(concept)\n",
    "            else:\n",
    "                if any(term.lower() in query_lower for term in related_terms):\n",
    "                    mentioned_concepts.append(concept)\n",
    "\n",
    "        # Extract keywords (excluding stopwords)\n",
    "        stop_words = {\"le\", \"la\", \"les\", \"un\", \"une\", \"des\", \"de\", \"du\", \"et\", \"ou\", \"à\", \"en\"}\n",
    "        words = [word.lower() for word in re.findall(r'\\b\\w+\\b', query)\n",
    "                if word.lower() not in stop_words and len(word) > 2]\n",
    "\n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"article_num\": article_match.group(1) if article_match else None,\n",
    "            \"concepts\": mentioned_concepts,\n",
    "            \"keywords\": words\n",
    "        }\n",
    "\n",
    "    async def search(self, query: str, field: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Search for relevant documents using hybrid search\"\"\"\n",
    "        # Validate field exists\n",
    "        if field not in cache.texts:\n",
    "            raise ValueError(f\"Field '{field}' not found in documents\")\n",
    "\n",
    "        # Preprocess query\n",
    "        query_info = await self.preprocess_query(query)\n",
    "        \n",
    "        # Check for specific article request\n",
    "        if query_info[\"article_num\"] and field in cache.articles:\n",
    "            article_result = await self._find_specific_article(\n",
    "                query_info[\"article_num\"], field\n",
    "            )\n",
    "            if article_result:\n",
    "                general_results = await self._general_search(\n",
    "                    query, field, top_k-1\n",
    "                )\n",
    "                # Combine results ensuring uniqueness\n",
    "                combined = [article_result]\n",
    "                seen_texts = {article_result[\"text\"]}\n",
    "                for res in general_results:\n",
    "                    if res[\"text\"] not in seen_texts:\n",
    "                        combined.append(res)\n",
    "                        seen_texts.add(res[\"text\"])\n",
    "                return combined[:top_k]\n",
    "\n",
    "        # Perform general search\n",
    "        return await self._general_search(query, field, top_k)\n",
    "\n",
    "    async def _find_specific_article(self, article_num: str, field: str) -> Optional[Dict]:\n",
    "        \"\"\"Find article by number if exists\"\"\"\n",
    "        if field in cache.articles and article_num in cache.articles[field]:\n",
    "            article_data = cache.articles[field][article_num]\n",
    "            return {\n",
    "                \"text\": article_data[\"text\"],\n",
    "                \"score\": 1.0,\n",
    "                \"metadata\": article_data[\"metadata\"]\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    async def _general_search(self, query: str, field: str, top_k: int) -> List[Dict]:\n",
    "        \"\"\"Perform hybrid BM25 + semantic search\"\"\"\n",
    "        # Run both searches in parallel\n",
    "        bm25_results, semantic_results = await asyncio.gather(\n",
    "            self._bm25_search(query, field, top_k),\n",
    "            self._semantic_search(query, field, top_k)\n",
    "        )\n",
    "\n",
    "        # Combine results with weighted scores\n",
    "        combined_scores = {}\n",
    "        \n",
    "        # BM25 results (60% weight)\n",
    "        for idx, score in bm25_results:\n",
    "            combined_scores[idx] = 0.6 * score\n",
    "            \n",
    "        # Semantic results (40% weight)\n",
    "        for idx, score in semantic_results:\n",
    "            combined_scores[idx] = combined_scores.get(idx, 0) + 0.4 * score\n",
    "\n",
    "        # Sort by combined score\n",
    "        sorted_indices = sorted(combined_scores.keys(), \n",
    "                               key=lambda x: combined_scores[x], \n",
    "                               reverse=True)[:top_k]\n",
    "\n",
    "        # Prepare results with metadata\n",
    "        results = []\n",
    "        for idx in sorted_indices:\n",
    "            if idx < len(cache.texts[field]):\n",
    "                results.append({\n",
    "                    \"text\": cache.texts[field][idx],\n",
    "                    \"score\": float(combined_scores[idx]),\n",
    "                    \"metadata\": cache.metadata[field][idx] if idx < len(cache.metadata[field]) else {}\n",
    "                })\n",
    "\n",
    "        return results\n",
    "\n",
    "    async def _bm25_search(self, query: str, field: str, top_k: int) -> List[Tuple[int, float]]:\n",
    "        \"\"\"BM25 keyword search\"\"\"\n",
    "        bm25 = cache.bm25[field]\n",
    "        scores = bm25.get_scores(query.split())\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        return [(idx, scores[idx]) for idx in top_indices]\n",
    "\n",
    "    async def _semantic_search(self, query: str, field: str, top_k: int) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Semantic vector search\"\"\"\n",
    "        # Ensure embeddings exist\n",
    "        if field not in cache.embeddings:\n",
    "            cache.embeddings[field] = await model_manager.get_embeddings(cache.texts[field])\n",
    "\n",
    "        # Get query embedding\n",
    "        query_embedding = (await model_manager.get_embeddings([query]))[0]\n",
    "        doc_embeddings = cache.embeddings[field]\n",
    "\n",
    "        # Calculate cosine similarities\n",
    "        similarities = np.zeros(len(doc_embeddings))\n",
    "        for i, emb in enumerate(doc_embeddings):\n",
    "            norm = np.linalg.norm(query_embedding) * np.linalg.norm(emb)\n",
    "            if norm > 0:\n",
    "                similarities[i] = np.dot(query_embedding, emb) / norm\n",
    "\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(idx, similarities[idx]) for idx in top_indices]\n",
    "\n",
    "    async def create_prompt(self, query: str, retrieved_docs: List[Dict], query_info: Dict, language: str = \"fr\") -> str:\n",
    "        \"\"\"Create optimized prompt for generation\"\"\"\n",
    "        # Format context with document references\n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            # Add article info if available\n",
    "            article_info = \"\"\n",
    "            meta = doc.get(\"metadata\", {})\n",
    "            if \"article\" in meta:\n",
    "                article_info = f\" (Article {meta['article']})\"\n",
    "            elif \"id\" in meta and \"article\" in meta[\"id\"]:\n",
    "                article_match = re.search(r'article_(\\d+)', meta[\"id\"])\n",
    "                if article_match:\n",
    "                    article_info = f\" (Article {article_match.group(1)})\"\n",
    "\n",
    "            context_parts.append(f\"Document {i+1}{article_info}:\\n{doc['text']}\")\n",
    "\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "        # Language-specific prompt engineering\n",
    "        if language == \"fr\":\n",
    "            # Article-specific guidance\n",
    "            article_specific = \"\"\n",
    "            if query_info[\"article_num\"]:\n",
    "                article_specific = (\n",
    "                    f\"La question concerne spécifiquement l'Article {query_info['article_num']}. \"\n",
    "                    \"Si cet article ne traite pas du sujet demandé, précisez-le clairement.\"\n",
    "                )\n",
    "\n",
    "            # Legal concept guidance\n",
    "            concept_guidance = \"\"\n",
    "            if query_info[\"concepts\"]:\n",
    "                concepts_list = \", \".join(query_info[\"concepts\"])\n",
    "                concept_guidance = (\n",
    "                    f\"Concepts juridiques pertinents: {concepts_list}. \"\n",
    "                    \"Analysez leur présence dans les documents.\"\n",
    "                )\n",
    "\n",
    "            prompt = (\n",
    "                f\"Question : {query}\\n\\n\"\n",
    "                f\"Contexte juridique :\\n{context}\\n\\n\"\n",
    "                \"Instructions :\\n\"\n",
    "                \"1. Répondez précisément à la question en vous basant sur le contexte\\n\"\n",
    "                \"2. Citez les articles pertinents\\n\"\n",
    "                \"3. Soyez clair et concis\\n\"\n",
    "                \"4. Si l'information est absente, indiquez-le\\n\\n\"\n",
    "                f\"{article_specific}\\n\"\n",
    "                f\"{concept_guidance}\\n\"\n",
    "                \"Réponse :\"\n",
    "            )\n",
    "        else:\n",
    "            # English version\n",
    "            article_specific = \"\"\n",
    "            if query_info[\"article_num\"]:\n",
    "                article_specific = (\n",
    "                    f\"The question specifically asks about Article {query_info['article_num']}. \"\n",
    "                    \"If this article doesn't address the topic, state this clearly.\"\n",
    "                )\n",
    "\n",
    "            concept_guidance = \"\"\n",
    "            if query_info[\"concepts\"]:\n",
    "                concepts_list = \", \".join(query_info[\"concepts\"])\n",
    "                concept_guidance = (\n",
    "                    f\"Relevant legal concepts: {concepts_list}. \"\n",
    "                    \"Analyze their presence in the documents.\"\n",
    "                )\n",
    "\n",
    "            prompt = (\n",
    "                f\"Question: {query}\\n\\n\"\n",
    "                f\"Legal Context:\\n{context}\\n\\n\"\n",
    "                \"Instructions:\\n\"\n",
    "                \"1. Answer the question precisely based on the context\\n\"\n",
    "                \"2. Cite relevant articles\\n\"\n",
    "                \"3. Be clear and concise\\n\"\n",
    "                \"4. If information is missing, state so\\n\\n\"\n",
    "                f\"{article_specific}\\n\"\n",
    "                f\"{concept_guidance}\\n\"\n",
    "                \"Answer:\"\n",
    "            )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    async def post_process_response(self, response: str, query_info: Dict,\n",
    "                                 retrieved_docs: List[Dict], language: str = \"fr\") -> str:\n",
    "        \"\"\"Validate and improve the generated response\"\"\"\n",
    "        # Check if specific article was requested but not properly addressed\n",
    "        if query_info.get(\"article_num\"):\n",
    "            article_ref = f\"Article {query_info['article_num']}\"\n",
    "            article_found = any(\n",
    "                doc.get(\"metadata\", {}).get(\"article\") == query_info[\"article_num\"]\n",
    "                or f\"article_{query_info['article_num']}\" in doc.get(\"metadata\", {}).get(\"id\", \"\")\n",
    "                for doc in retrieved_docs\n",
    "            )\n",
    "\n",
    "            if article_found and article_ref not in response:\n",
    "                concepts = \" et \".join(query_info[\"concepts\"]) if query_info[\"concepts\"] else \"le sujet demandé\"\n",
    "                \n",
    "                if language == \"fr\":\n",
    "                    disclaimer = (\n",
    "                        f\"\\n\\nNote: L'{article_ref} a été analysé mais ne contient pas \"\n",
    "                        f\"de dispositions spécifiques concernant {concepts}.\"\n",
    "                    )\n",
    "                else:\n",
    "                    disclaimer = (\n",
    "                        f\"\\n\\nNote: {article_ref} was analyzed but doesn't contain \"\n",
    "                        f\"specific provisions regarding {concepts}.\"\n",
    "                    )\n",
    "\n",
    "                if not any(term in response.lower() for term in\n",
    "                          [\"ne contient pas\", \"ne mentionne pas\", \"ne traite pas\",\n",
    "                           \"does not contain\", \"does not mention\", \"does not address\"]):\n",
    "                    response += disclaimer\n",
    "\n",
    "        return response.strip()\n",
    "\n",
    "    async def generate_reflection(self, query_id: str, reflection_prompt: str) -> str:\n",
    "        \"\"\"Generate self-reflection on a previous response\n",
    "        \"\"\"\n",
    "        if query_id not in cache.query_metadata:\n",
    "            raise ValueError(\"Query ID not found\")\n",
    "\n",
    "        metadata = cache.query_metadata[query_id]\n",
    "\n",
    "        full_prompt = (\n",
    "            f\"{reflection_prompt}\\n\\n\"\n",
    "            f\"Original Question: {metadata['query_text']}\\n\"\n",
    "            f\"Generated Response: {metadata['final_response']}\\n\\n\"\n",
    "            \"Context Documents:\\n\"\n",
    "            + \"\\n---\\n\".join([doc[\"text\"] for doc in metadata[\"retrieved_documents\"]])\n",
    "            + \"\\n\\nAnalysis:\"\n",
    "        )\n",
    "\n",
    "        return self.model_manager.generate_response(\n",
    "            prompt=full_prompt,\n",
    "            max_tokens=300,\n",
    "            temperature=0.3  # Lower temperature for more focused reflection\n",
    "        )\n",
    "\n",
    "# ================= API Endpoints =================\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"Initialize the application\"\"\"\n",
    "    try:\n",
    "        # In your FastAPI startup\n",
    "        logger.info(f\"Current query IDs in cache: {list(cache.query_metadata.keys())}\")\n",
    "        await model_manager.load_models()\n",
    "        await document_manager.load_documents()\n",
    "        logger.info(\"API startup completed\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Startup failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@app.post(\"/query\", response_model=QueryResponse)\n",
    "async def query_legal_assistant(request: QueryRequest, \n",
    "                              background_tasks: BackgroundTasks):\n",
    "    \"\"\"Main query endpoint\"\"\"\n",
    "    start_time = time.time()\n",
    "    query_id = str(uuid.uuid4())\n",
    "\n",
    "    try:\n",
    "        # Validate field\n",
    "        if request.field not in DOC_CONFIG:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_400_BAD_REQUEST,\n",
    "                detail=f\"Invalid field. Choose from: {list(DOC_CONFIG.keys())}\"\n",
    "            )\n",
    "\n",
    "        # Ensure documents are loaded\n",
    "        if not await document_manager.load_documents():\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "                detail=\"Failed to load documents\"\n",
    "            )\n",
    "\n",
    "        # Initialize processor\n",
    "        processor = QueryProcessor(model_manager)\n",
    "\n",
    "        # Process query\n",
    "        query_info = await processor.preprocess_query(request.query)\n",
    "        retrieved_docs = await processor.search(\n",
    "            query=request.query,\n",
    "            field=request.field,\n",
    "            top_k=request.top_k\n",
    "        )\n",
    "\n",
    "        # Handle empty results\n",
    "        if not retrieved_docs:\n",
    "            empty_response = {\n",
    "                \"answer\": \"Aucune information pertinente trouvée dans les documents.\" \n",
    "                          if request.language == \"fr\" else \n",
    "                          \"No relevant information found in documents.\",\n",
    "                \"retrieved_documents\": [],\n",
    "                \"query_time_ms\": (time.time() - start_time) * 1000,\n",
    "                \"query_id\": query_id,\n",
    "                \"cache_hit\": False\n",
    "            }\n",
    "            return empty_response\n",
    "\n",
    "        # Generate response\n",
    "        prompt = await processor.create_prompt(\n",
    "            query=request.query,\n",
    "            retrieved_docs=retrieved_docs,\n",
    "            query_info=query_info,\n",
    "            language=request.language\n",
    "        )\n",
    "\n",
    "        raw_response = model_manager.generate_response(\n",
    "            prompt=prompt,\n",
    "            max_tokens=request.max_tokens,\n",
    "            temperature=request.temperature\n",
    "        )\n",
    "\n",
    "        processed_response = await processor.post_process_response(\n",
    "            response=raw_response,\n",
    "            query_info=query_info,\n",
    "            retrieved_docs=retrieved_docs,\n",
    "            language=request.language\n",
    "        )\n",
    "\n",
    "        # Store query metadata BEFORE generating reflection\n",
    "        query_metadata = {\n",
    "            \"query_id\": query_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query_text\": request.query,\n",
    "            \"field\": request.field,\n",
    "            \"language\": request.language,\n",
    "            \"prompt\": prompt,\n",
    "            \"retrieved_documents\": [doc.copy() for doc in retrieved_docs],\n",
    "            \"raw_response\": raw_response,\n",
    "            \"final_response\": processed_response\n",
    "        }\n",
    "        \n",
    "        cache.query_metadata[query_id] = query_metadata\n",
    "\n",
    "        # Generate reflection if enabled\n",
    "        reflection = None\n",
    "        if request.enable_reflection:\n",
    "            reflection = await processor.generate_reflection(\n",
    "                query_id=query_id,\n",
    "                reflection_prompt=\"Analyze this legal response for accuracy and completeness:\"\n",
    "            )\n",
    "            query_metadata[\"reflection\"] = reflection\n",
    "\n",
    "        # Prepare response\n",
    "        response_data = {\n",
    "            \"answer\": processed_response,\n",
    "            \"retrieved_documents\": retrieved_docs,\n",
    "            \"query_time_ms\": (time.time() - start_time) * 1000,\n",
    "            \"query_id\": query_id,\n",
    "            \"reflection\": reflection,\n",
    "            \"cache_hit\": False\n",
    "        }\n",
    "\n",
    "        return response_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Query failed: {str(e)}\")\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=str(e)\n",
    "        )\n",
    "\n",
    "@app.post(\"/feedback\", status_code=status.HTTP_201_CREATED)\n",
    "async def submit_feedback(feedback: FeedbackRequest):\n",
    "    \"\"\"Endpoint for submitting feedback on responses\"\"\"\n",
    "    try:\n",
    "        # Validate query exists\n",
    "        if feedback.query_id not in cache.query_metadata:\n",
    "            logger.error(f\"Query ID {feedback.query_id} not found in metadata\")\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_404_NOT_FOUND,\n",
    "                detail=\"Query ID not found in system records\"\n",
    "            )\n",
    "\n",
    "        # Store feedback\n",
    "        feedback_data = {\n",
    "            \"type\": feedback.feedback_type,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"correction\": feedback.correction_text if feedback.feedback_type == FeedbackType.CORRECTION else None,\n",
    "            \"comments\": feedback.comments\n",
    "        }\n",
    "\n",
    "        cache.feedback[feedback.query_id] = feedback_data\n",
    "        cache.query_metadata[feedback.query_id][\"feedback\"] = feedback_data\n",
    "\n",
    "        # In production: Trigger fine-tuning pipeline here\n",
    "\n",
    "        return {\"status\": \"success\", \"message\": \"Feedback recorded\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Feedback failed: {str(e)}\")\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=str(e)\n",
    "        )\n",
    "\n",
    "@app.post(\"/reflect\", response_model=Dict)\n",
    "async def reflect_on_response(request: ReflectionRequest):\n",
    "    \"\"\"Endpoint for generating self-reflection on a response\"\"\"\n",
    "    try:\n",
    "        if request.query_id not in cache.query_metadata:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_404_NOT_FOUND,\n",
    "                detail=\"Query ID not found\"\n",
    "            )\n",
    "\n",
    "        processor = QueryProcessor(model_manager)\n",
    "        reflection = await processor.generate_reflection(\n",
    "            query_id=request.query_id,\n",
    "            reflection_prompt=request.reflection_prompt\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"reflection\": reflection,\n",
    "            \"query_id\": request.query_id\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Reflection failed: {str(e)}\")\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=str(e)\n",
    "        )\n",
    "\n",
    "@app.get(\"/stats\", response_model=Dict)\n",
    "async def get_system_stats():\n",
    "    \"\"\"Endpoint for system statistics\"\"\"\n",
    "    return {\n",
    "        \"documents_loaded\": {\n",
    "            field: len(texts) for field, texts in cache.texts.items()\n",
    "        },\n",
    "        \"queries_processed\": len(cache.query_metadata),\n",
    "        \"feedback_received\": len(cache.feedback),\n",
    "        \"feedback_stats\": {\n",
    "            \"positive\": sum(1 for f in cache.feedback.values() if f[\"type\"] == \"positive\"),\n",
    "            \"negative\": sum(1 for f in cache.feedback.values() if f[\"type\"] == \"negative\"),\n",
    "            \"corrections\": sum(1 for f in cache.feedback.values() if f[\"type\"] == \"correction\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.post(\"/reload\", status_code=status.HTTP_200_OK)\n",
    "async def reload_documents():\n",
    "    \"\"\"Reload documents from source\"\"\"\n",
    "    try:\n",
    "        success = await document_manager.load_documents(force_reload=True)\n",
    "        if success:\n",
    "            return {\"status\": \"success\", \"message\": \"Documents reloaded\"}\n",
    "        else:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "                detail=\"Failed to reload documents\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Document reload failed: {str(e)}\")\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=str(e)\n",
    "        )\n",
    "\n",
    "# Initialize components\n",
    "model_manager = ModelManager(MODEL_CONFIG)\n",
    "document_manager = DocumentManager(DOC_CONFIG)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import nest_asyncio\n",
    "    from uvicorn import Config, Server\n",
    "\n",
    "    # Apply nest_asyncio to allow nested event loops\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Configure and run the Uvicorn server\n",
    "    config = Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = Server(config)\n",
    "    server.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [16996]\n",
      "INFO:     Waiting for application startup.\n",
      "2025-03-29 20:17:53,663 - __main__ - INFO - Loading models from google/flan-t5-large...\n",
      "2025-03-29 20:17:56,332 - __main__ - INFO - Loading sentence transformer model...\n",
      "2025-03-29 20:17:56,337 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
      "2025-03-29 20:17:56,340 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-03-29 20:18:00,597 - asyncio - ERROR - Task exception was never retrieved\n",
      "future: <Task finished name='Task-163' coro=<Server.serve() done, defined at c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\uvicorn\\server.py:68> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\uvicorn\\main.py\", line 579, in run\n",
      "    server.run()\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\uvicorn\\server.py\", line 66, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\asyncio\\tasks.py\", line 339, in __wakeup\n",
      "    self.__step()\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\asyncio\\tasks.py\", line 256, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\uvicorn\\server.py\", line 70, in serve\n",
      "    await self._serve(sockets)\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\contextlib.py\", line 126, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"c:\\Users\\mouni\\anaconda3\\envs\\tun_law_env\\lib\\site-packages\\uvicorn\\server.py\", line 330, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n",
      "2025-03-29 20:18:04,772 - __main__ - INFO - Models loaded successfully\n",
      "2025-03-29 20:18:04,774 - __main__ - INFO - Loading and processing documents...\n",
      "2025-03-29 20:18:04,781 - __main__ - INFO - Loading document for field: criminal\n",
      "Batches: 100%|██████████| 15/15 [00:11<00:00,  1.30it/s]\n",
      "2025-03-29 20:18:17,907 - __main__ - INFO - Loaded 460 regular chunks from criminal\n",
      "2025-03-29 20:18:17,911 - __main__ - INFO - Extracted 0 articles from criminal\n",
      "2025-03-29 20:18:17,913 - __main__ - INFO - Loading document for field: constitution\n",
      "Batches: 100%|██████████| 8/8 [00:06<00:00,  1.26it/s]\n",
      "2025-03-29 20:18:25,166 - __main__ - INFO - Loaded 252 regular chunks from constitution\n",
      "2025-03-29 20:18:25,169 - __main__ - INFO - Extracted 148 articles from constitution\n",
      "2025-03-29 20:18:25,170 - __main__ - INFO - Documents loaded successfully. Available fields: ['criminal', 'constitution']\n",
      "2025-03-29 20:18:25,171 - __main__ - INFO - API started successfully\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n",
      "2025-03-29 20:20:26,195 - __main__ - INFO - Searching in field: constitution\n",
      "2025-03-29 20:20:26,197 - __main__ - INFO - Field 'constitution' contains 252 text chunks and 148 articles\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:50147 - \"POST /query HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [16996]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import time\n",
    "from functools import lru_cache\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from rank_bm25 import BM25Okapi\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional, Set\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uvicorn\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"api.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Tunisian Legal Assistant API\",\n",
    "    description=\"An API for querying Tunisian legal documents using AI\",\n",
    "    version=\"1.1.0\"\n",
    ")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    \"model_id\": \"google/flan-t5-large\",  # Consider upgrading to a better model\n",
    "    \"cache_dir\": \"./model_cache\",\n",
    "    \"quantize\": False,\n",
    "    \"use_gpu\": True if torch.cuda.is_available() else False,\n",
    "    \"max_length\": 512\n",
    "}\n",
    "\n",
    "# Document configuration\n",
    "DOC_CONFIG = {\n",
    "    \"criminal\": {\n",
    "        \"path\": r\"C:\\Users\\mouni\\tun_law_project\\Tunisia_Crim-Fr.pdf\",\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 100\n",
    "    },\n",
    "    \"constitution\": {\n",
    "        \"path\": r\"C:\\Users\\mouni\\tun_law_project\\Constitution_fr.pdf\",\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 100\n",
    "    }\n",
    "}\n",
    "\n",
    "# Legal terminology and synonyms for improved search\n",
    "LEGAL_TERMS = {\n",
    "    \"imprescriptibilité\": [\"prescription\", \"délai\", \"expiration\", \"crime\", \"poursuites\"],\n",
    "    \"droit\": [\"liberté\", \"garantie\", \"protection\"],\n",
    "    \"justice\": [\"tribunal\", \"magistrat\", \"judiciaire\"],\n",
    "    # Add more legal concepts and related terms\n",
    "}\n",
    "\n",
    "# Cache for document data and responses\n",
    "cache = {\n",
    "    \"texts\": {},\n",
    "    \"bm25\": {},\n",
    "    \"embeddings\": {},\n",
    "    \"articles\": {},\n",
    "    \"last_loaded\": None,\n",
    "    \"responses\": {}\n",
    "}\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipeline = None\n",
    "        self.embedding_model = None\n",
    "\n",
    "    async def load_models(self):\n",
    "        logger.info(f\"Loading models from {self.config['model_id']}...\")\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config[\"model_id\"],\n",
    "            cache_dir=self.config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        # Load LLM\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            self.config[\"model_id\"],\n",
    "            cache_dir=self.config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        # Apply quantization if enabled\n",
    "        if self.config[\"quantize\"]:\n",
    "            logger.info(\"Applying quantization...\")\n",
    "            self.model = torch.quantization.quantize_dynamic(\n",
    "                self.model,\n",
    "                {torch.nn.Linear},\n",
    "                dtype=torch.float16\n",
    "            )\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(\"Moving model to GPU...\")\n",
    "            self.model.to('cuda')\n",
    "\n",
    "        # Create generation pipeline\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        self.pipeline = pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Load sentence transformer for semantic search\n",
    "        logger.info(\"Loading sentence transformer model...\")\n",
    "        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        if torch.cuda.is_available():\n",
    "            self.embedding_model = self.embedding_model.to(torch.device(\"cuda\"))\n",
    "\n",
    "        logger.info(\"Models loaded successfully\")\n",
    "\n",
    "    @lru_cache(maxsize=100)\n",
    "    def generate_response(self, prompt, max_tokens=150, temperature=0.7):\n",
    "        \"\"\"Generate response with caching for identical prompts\"\"\"\n",
    "        if not self.pipeline:\n",
    "            raise ValueError(\"Model pipeline not initialized\")\n",
    "\n",
    "        response = self.pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            num_return_sequences=1,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "\n",
    "        # Extract only the newly generated text\n",
    "        generated_text = response[0][\"generated_text\"]\n",
    "        if generated_text.startswith(prompt):\n",
    "            generated_text = generated_text[len(prompt):].strip()\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "    async def get_embeddings(self, texts):\n",
    "        \"\"\"Generate embeddings for a list of texts using the sentence transformer model\"\"\"\n",
    "        if not self.embedding_model:\n",
    "            raise ValueError(\"Embedding model not initialized\")\n",
    "\n",
    "        embeddings = self.embedding_model.encode(texts, convert_to_tensor=True)\n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "class DocumentManager:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    async def load_documents(self, force_reload=False):\n",
    "        global cache\n",
    "        success = False\n",
    "\n",
    "        current_time = datetime.now()\n",
    "\n",
    "        # Skip loading if documents were loaded within the last hour\n",
    "        if not force_reload and cache.get(\"last_loaded\") and \\\n",
    "           (current_time - cache[\"last_loaded\"]).total_seconds() < 3600 and \\\n",
    "           cache.get(\"texts\") and len(cache[\"texts\"]) > 0:\n",
    "            logger.info(\"Using cached document data\")\n",
    "            return True\n",
    "\n",
    "        logger.info(\"Loading and processing documents...\")\n",
    "\n",
    "        # Initialize caches\n",
    "        cache.setdefault(\"texts\", {})\n",
    "        cache.setdefault(\"bm25\", {})\n",
    "        cache.setdefault(\"embeddings\", {})\n",
    "        cache.setdefault(\"articles\", {})\n",
    "\n",
    "        for field, field_config in self.config.items():\n",
    "            pdf_path = field_config[\"path\"]\n",
    "            chunk_size = field_config[\"chunk_size\"]\n",
    "            chunk_overlap = field_config[\"chunk_overlap\"]\n",
    "\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(pdf_path):\n",
    "                logger.error(f\"Document file not found: {pdf_path}\")\n",
    "                continue\n",
    "\n",
    "            # Load document\n",
    "            try:\n",
    "                logger.info(f\"Loading document for field: {field}\")\n",
    "                # Load and process documents with article extraction\n",
    "                await self._process_document(pdf_path, field, chunk_size, chunk_overlap)\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading document {pdf_path}: {str(e)}\")\n",
    "                # Continue with other documents instead of failing completely\n",
    "\n",
    "        if success:\n",
    "            cache[\"last_loaded\"] = current_time\n",
    "            logger.info(f\"Documents loaded successfully. Available fields: {list(cache.get('texts', {}).keys())}\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(\"Failed to load any documents\")\n",
    "            return False\n",
    "\n",
    "    async def _process_document(self, pdf_path, field, chunk_size, chunk_overlap):\n",
    "        # Load PDF\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pages = loader.load()\n",
    "\n",
    "        # Process for different chunk types\n",
    "        await asyncio.gather(\n",
    "            self._process_regular_chunks(pages, field, chunk_size, chunk_overlap),\n",
    "            self._process_article_chunks(pages, field)\n",
    "        )\n",
    "\n",
    "    async def _process_regular_chunks(self, pages, field, chunk_size, chunk_overlap):\n",
    "        \"\"\"Process document into regular chunks for general search\"\"\"\n",
    "        # Split into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        documents = text_splitter.split_documents(pages)\n",
    "\n",
    "        # Extract page content and metadata\n",
    "        texts = []\n",
    "        metadata = []\n",
    "\n",
    "        for doc in documents:\n",
    "            texts.append(doc.page_content)\n",
    "            metadata.append({\n",
    "                \"page\": doc.metadata.get(\"page\", 0),\n",
    "                \"source\": os.path.basename(doc.metadata.get(\"source\", \"\")),\n",
    "                \"id\": f\"{field}_{doc.metadata.get('page', 0)}_{len(texts)}\"\n",
    "            })\n",
    "\n",
    "        # Store in cache\n",
    "        cache[\"texts\"][field] = texts\n",
    "        cache.setdefault(\"metadata\", {})\n",
    "        cache[\"metadata\"][field] = metadata\n",
    "\n",
    "        # Create BM25 index\n",
    "        tokenized_texts = [text.split() for text in texts]\n",
    "        cache[\"bm25\"][field] = BM25Okapi(tokenized_texts)\n",
    "\n",
    "        # Get embeddings in background\n",
    "        cache.setdefault(\"embeddings\", {})\n",
    "        cache[\"embeddings\"][field] = await model_manager.get_embeddings(texts)\n",
    "\n",
    "        logger.info(f\"Loaded {len(texts)} regular chunks from {field}\")\n",
    "\n",
    "    async def _process_article_chunks(self, pages, field):\n",
    "        \"\"\"Process document to extract article-specific chunks\"\"\"\n",
    "        # Compile regex for article detection\n",
    "        article_pattern = re.compile(r'Article\\s+(\\d+)\\s*:', re.IGNORECASE)\n",
    "\n",
    "        # Initialize article cache for this field\n",
    "        cache.setdefault(\"articles\", {})\n",
    "        cache[\"articles\"].setdefault(field, {})\n",
    "\n",
    "        for page in pages:\n",
    "            content = page.page_content\n",
    "            article_matches = list(article_pattern.finditer(content))\n",
    "\n",
    "            for i, match in enumerate(article_matches):\n",
    "                article_num = match.group(1)\n",
    "                start_pos = match.start()\n",
    "\n",
    "                # Find the next article or end of page\n",
    "                end_pos = article_matches[i+1].start() if i < len(article_matches) - 1 else len(content)\n",
    "\n",
    "                article_text = content[start_pos:end_pos].strip()\n",
    "\n",
    "                # Store with article number as key\n",
    "                cache[\"articles\"][field][article_num] = {\n",
    "                    \"text\": article_text,\n",
    "                    \"metadata\": {\n",
    "                        \"page\": page.metadata.get(\"page\", 0),\n",
    "                        \"source\": os.path.basename(page.metadata.get(\"source\", \"\")),\n",
    "                        \"id\": f\"{field}_article_{article_num}\",\n",
    "                        \"article\": int(article_num)\n",
    "                    }\n",
    "                }\n",
    "\n",
    "        logger.info(f\"Extracted {len(cache['articles'][field])} articles from {field}\")\n",
    "\n",
    "\n",
    "class QueryProcessor:\n",
    "    def __init__(self, model_manager):\n",
    "        self.model_manager = model_manager\n",
    "\n",
    "    async def preprocess_query(self, query):\n",
    "        \"\"\"Extract key information from the query\"\"\"\n",
    "        # Check for article references\n",
    "        article_match = re.search(r'article\\s+(\\d+)', query, re.IGNORECASE)\n",
    "        article_num = article_match.group(1) if article_match else None\n",
    "\n",
    "        # Extract legal concepts mentioned\n",
    "        query_lower = query.lower()\n",
    "        mentioned_concepts = []\n",
    "        for concept, related_terms in LEGAL_TERMS.items():\n",
    "            if concept.lower() in query_lower:\n",
    "                mentioned_concepts.append(concept)\n",
    "            else:\n",
    "                # Check for related terms\n",
    "                for term in related_terms:\n",
    "                    if term.lower() in query_lower:\n",
    "                        mentioned_concepts.append(concept)\n",
    "                        break\n",
    "\n",
    "        # Extract query keywords (simple approach)\n",
    "        stop_words = {\"le\", \"la\", \"les\", \"un\", \"une\", \"des\", \"de\", \"du\", \"et\", \"ou\", \"à\", \"en\"}\n",
    "        words = [word.lower() for word in re.findall(r'\\b\\w+\\b', query)\n",
    "                if word.lower() not in stop_words and len(word) > 2]\n",
    "\n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"article_num\": article_num,\n",
    "            \"concepts\": mentioned_concepts,\n",
    "            \"keywords\": words\n",
    "        }\n",
    "\n",
    "    async def search(self, query, field, top_k=2):\n",
    "        \"\"\"Search for relevant documents in the specified field\"\"\"\n",
    "        logger.info(f\"Searching in field: {field}\")\n",
    "        \n",
    "        # Check if documents are loaded\n",
    "        if not cache.get(\"texts\"):\n",
    "            logger.error(\"No documents loaded in cache\")\n",
    "            raise HTTPException(status_code=500, detail=\"Document cache is empty. Try reloading documents.\")\n",
    "        \n",
    "        # Handle case where field doesn't exist\n",
    "        if field not in cache.get(\"texts\", {}):\n",
    "            logger.error(f\"Field '{field}' not found in loaded documents. Available fields: {list(cache.get('texts', {}).keys())}\")\n",
    "            raise HTTPException(\n",
    "                status_code=400, \n",
    "                detail=f\"Field '{field}' not found in loaded documents. Available fields: {list(cache.get('texts', {}).keys())}\"\n",
    "            )\n",
    "        \n",
    "        # Preprocess query\n",
    "        query_info = await self.preprocess_query(query)\n",
    "        \n",
    "        # Log field info for debugging\n",
    "        logger.info(f\"Field '{field}' contains {len(cache['texts'][field])} text chunks and {len(cache.get('articles', {}).get(field, {}))} articles\")\n",
    "        \n",
    "        # Specific article search if requested\n",
    "        if query_info[\"article_num\"] and field in cache.get(\"articles\", {}):\n",
    "            article_results = await self._find_specific_article(\n",
    "                query_info[\"article_num\"], field\n",
    "            )\n",
    "\n",
    "            if article_results:\n",
    "                # Combine with general search results if needed\n",
    "                general_results = await self._general_search(\n",
    "                    query, field, top_k-1\n",
    "                )\n",
    "\n",
    "                # Remove duplicates and limit to top_k\n",
    "                combined = [article_results]\n",
    "                for res in general_results:\n",
    "                    if res[\"text\"] != article_results[\"text\"]:\n",
    "                        combined.append(res)\n",
    "\n",
    "                return combined[:top_k]\n",
    "\n",
    "        # General search if no article specified or article not found\n",
    "        results = await self._general_search(query, field, top_k)\n",
    "        return results\n",
    "\n",
    "\n",
    "    async def _find_specific_article(self, article_num, field):\n",
    "        \"\"\"Find a specific article by number\"\"\"\n",
    "        if field in cache.get(\"articles\", {}) and article_num in cache[\"articles\"].get(field, {}):\n",
    "            article_data = cache[\"articles\"][field][article_num]\n",
    "            return {\n",
    "                \"text\": article_data[\"text\"],\n",
    "                \"score\": 1.0,  # Highest score for exact match\n",
    "                \"metadata\": article_data[\"metadata\"]\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    async def _general_search(self, query, field, top_k=3):\n",
    "        if field not in cache.get(\"texts\", {}):\n",
    "            logger.error(f\"Field '{field}' not found in texts cache. Available fields: {list(cache.get('texts', {}).keys())}\")\n",
    "            raise ValueError(f\"Field '{field}' not found in loaded documents\")\n",
    "            \n",
    "        texts = cache[\"texts\"][field]\n",
    "\n",
    "        # Run BM25 and embedding search in parallel\n",
    "        bm25_results, semantic_results = await asyncio.gather(\n",
    "            self._bm25_search(query, field, top_k),\n",
    "            self._semantic_search(query, field, top_k)\n",
    "        )\n",
    "\n",
    "        # Combine results with weights\n",
    "        combined_indices = set()\n",
    "        combined_scores = {}\n",
    "\n",
    "        # BM25 results (60% weight)\n",
    "        for idx, score in bm25_results:\n",
    "            combined_indices.add(idx)\n",
    "            combined_scores[idx] = 0.6 * score\n",
    "\n",
    "        # Semantic results (40% weight)\n",
    "        for idx, score in semantic_results:\n",
    "            combined_indices.add(idx)\n",
    "            combined_scores[idx] = combined_scores.get(idx, 0) + 0.4 * score\n",
    "\n",
    "        # Create the combined results\n",
    "        retrieved_texts = []\n",
    "        for idx in combined_indices:\n",
    "            if idx < len(texts):\n",
    "                retrieved_texts.append({\n",
    "                    \"text\": texts[idx],\n",
    "                    \"score\": float(combined_scores[idx]),\n",
    "                    \"metadata\": cache.get(\"metadata\", {}).get(field, [])[idx] if idx < len(cache.get(\"metadata\", {}).get(field, [])) else {}\n",
    "                })\n",
    "\n",
    "        # Sort by combined score\n",
    "        retrieved_texts.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "        return retrieved_texts[:top_k]\n",
    "\n",
    "    async def _bm25_search(self, query, field, top_k):\n",
    "        \"\"\"BM25 search function\"\"\"\n",
    "        if field not in cache.get(\"bm25\", {}):\n",
    "            logger.error(f\"Field '{field}' not found in BM25 cache. Available fields: {list(cache.get('bm25', {}).keys())}\")\n",
    "            raise ValueError(f\"BM25 index not found for field '{field}'\")\n",
    "            \n",
    "        bm25 = cache[\"bm25\"][field]\n",
    "        bm25_scores = bm25.get_scores(query.split())\n",
    "        top_indices = np.argsort(bm25_scores)[-top_k:][::-1]\n",
    "\n",
    "        return [(idx, bm25_scores[idx]) for idx in top_indices]\n",
    "\n",
    "    async def _semantic_search(self, query, field, top_k):\n",
    "        \"\"\"Semantic search using embeddings\"\"\"\n",
    "        # Ensure embeddings exist\n",
    "        if field not in cache.get(\"embeddings\", {}):\n",
    "            logger.warning(f\"No embeddings for {field}, calculating now\")\n",
    "            if field not in cache.get(\"texts\", {}):\n",
    "                logger.error(f\"Field '{field}' not found in texts cache. Available fields: {list(cache.get('texts', {}).keys())}\")\n",
    "                raise ValueError(f\"Cannot create embeddings for field '{field}' - field not found in text cache\")\n",
    "                \n",
    "            cache.setdefault(\"embeddings\", {})\n",
    "            cache[\"embeddings\"][field] = await self.model_manager.get_embeddings(cache[\"texts\"][field])\n",
    "\n",
    "        # Get query embedding\n",
    "        query_embedding = (await self.model_manager.get_embeddings([query]))[0]\n",
    "        doc_embeddings = cache[\"embeddings\"][field]\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarities = np.zeros(len(doc_embeddings))\n",
    "        for i, embedding in enumerate(doc_embeddings):\n",
    "            similarities[i] = np.dot(query_embedding, embedding) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(embedding)\n",
    "            )\n",
    "\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(idx, similarities[idx]) for idx in top_indices]\n",
    "\n",
    "    async def create_prompt(self, query, retrieved_documents, query_info, language=\"fr\"):\n",
    "        \"\"\"Create an optimized prompt based on query analysis and retrieved documents\"\"\"\n",
    "        # Format context with clear article references\n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(retrieved_documents):\n",
    "            # Check if this is an article and extract the number\n",
    "            article_info = \"\"\n",
    "            if \"metadata\" in doc:\n",
    "                if \"article\" in doc[\"metadata\"]:\n",
    "                    article_info = f\" (Article {doc['metadata']['article']})\"\n",
    "                elif \"id\" in doc[\"metadata\"] and \"article\" in doc[\"metadata\"][\"id\"]:\n",
    "                    article_match = re.search(r'article_(\\d+)', doc[\"metadata\"][\"id\"])\n",
    "                    if article_match:\n",
    "                        article_info = f\" (Article {article_match.group(1)})\"\n",
    "\n",
    "            context_parts.append(f\"Document {i+1}{article_info}:\\n{doc['text']}\")\n",
    "\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "        if language == \"fr\":\n",
    "            # Check if query asked for a specific article\n",
    "            article_specific = \"\"\n",
    "            if query_info[\"article_num\"]:\n",
    "                article_specific = (\n",
    "                    f\"La question porte spécifiquement sur l'Article {query_info['article_num']}. \"\n",
    "                    \"Si cet article ne traite pas du sujet demandé, précisez-le clairement.\"\n",
    "                )\n",
    "\n",
    "            # Check if query mentioned specific legal concepts\n",
    "            concept_guidance = \"\"\n",
    "            if query_info[\"concepts\"]:\n",
    "                concepts_list = \", \".join(query_info[\"concepts\"])\n",
    "                concept_guidance = (\n",
    "                    f\"La question porte sur les concepts juridiques suivants: {concepts_list}. \"\n",
    "                    \"Analysez si ces concepts sont présents dans les documents fournis.\"\n",
    "                )\n",
    "\n",
    "            prompt = (\n",
    "                f\"Question : {query}\\n\\n\"\n",
    "                f\"Contexte juridique :\\n{context}\\n\\n\"\n",
    "                \"Instructions : répondre a la question d'aprés le contexte.\\n\\n\"\n",
    "                f\"{article_specific}\\n\"\n",
    "                f\"{concept_guidance}\\n\"\n",
    "                # \"Votre réponse doit :\\n\"\n",
    "                # \"1. Vérifier si l'information est présente.\\n\"\n",
    "                # \"2. Ne pas inventer d'informations.\\n\"\n",
    "                # \"3. Citer les articles pertinents.\\n\"\n",
    "                # \"4. Être claire et structurée.\\n\"\n",
    "                # \"5. Indiquer si l'information manque.\\n\\n\"\n",
    "                \"Réponse :\"\n",
    "            )\n",
    "        else:\n",
    "            # English version\n",
    "            article_specific = \"\"\n",
    "            if query_info[\"article_num\"]:\n",
    "                article_specific = (\n",
    "                    f\"The question specifically asks about Article {query_info['article_num']}. \"\n",
    "                    \"If this article does not address the topic, clearly state this.\"\n",
    "                )\n",
    "\n",
    "            concept_guidance = \"\"\n",
    "            if query_info[\"concepts\"]:\n",
    "                concepts_list = \", \".join(query_info[\"concepts\"])\n",
    "                concept_guidance = (\n",
    "                    f\"The question concerns the following legal concepts: {concepts_list}. \"\n",
    "                    \"Analyze whether these concepts are present in the provided documents.\"\n",
    "                )\n",
    "\n",
    "            prompt = (\n",
    "                f\"Question : {query}\\n\\n\"\n",
    "                f\"Contexte juridique :\\n{context}\\n\\n\"\n",
    "                \"Instructions : répondre a la question d'aprés le contexte.\\n\\n\"\n",
    "                f\"{article_specific}\\n\"\n",
    "                f\"{concept_guidance}\\n\"\n",
    "                # \"Votre réponse doit :\\n\"\n",
    "                # \"1. Vérifier si l'information est présente.\\n\"\n",
    "                # \"2. Ne pas inventer d'informations.\\n\"\n",
    "                # \"3. Citer les articles pertinents.\\n\"\n",
    "                # \"4. Être claire et structurée.\\n\"\n",
    "                # \"5. Indiquer si l'information manque.\\n\\n\"\n",
    "                \"Réponse :\"\n",
    "            )\n",
    "            \n",
    "        return prompt\n",
    "\n",
    "    async def post_process_response(self, response, query_info, retrieved_docs, language=\"fr\"):\n",
    "        \"\"\"Clean up and validate the response\"\"\"\n",
    "        # Check if a specific article was requested but not found in response\n",
    "        if query_info.get(\"article_num\"):\n",
    "            article_reference = f\"Article {query_info['article_num']}\"\n",
    "            article_not_found = True\n",
    "\n",
    "            # Check if any retrieved document contains the requested article\n",
    "            for doc in retrieved_docs:\n",
    "                if \"metadata\" in doc:\n",
    "                    if \"article\" in doc[\"metadata\"] and str(doc[\"metadata\"][\"article\"]) == query_info[\"article_num\"]:\n",
    "                        article_not_found = False\n",
    "                        break\n",
    "                    if \"id\" in doc[\"metadata\"] and f\"article_{query_info['article_num']}\" in doc[\"metadata\"][\"id\"]:\n",
    "                        article_not_found = False\n",
    "                        break\n",
    "\n",
    "            # If article was found in documents but response doesn't clarify it doesn't contain requested info\n",
    "            if not article_not_found and article_reference not in response:\n",
    "                concepts = \" et \".join(query_info[\"concepts\"]) if query_info[\"concepts\"] else \"le sujet demandé\"\n",
    "\n",
    "                if language == \"fr\":\n",
    "                    disclaimer = (\n",
    "                        f\"\\n\\nNote: Bien que l'Article {query_info['article_num']} ait été analysé, \"\n",
    "                        f\"il ne contient pas de dispositions spécifiques concernant {concepts}.\"\n",
    "                    )\n",
    "                else:\n",
    "                    disclaimer = (\n",
    "                        f\"\\n\\nNote: Although Article {query_info['article_num']} was analyzed, \"\n",
    "                        f\"it does not contain specific provisions regarding {concepts}.\"\n",
    "                    )\n",
    "\n",
    "                # Only add disclaimer if response doesn't already indicate this\n",
    "                if not any(term in response.lower() for term in\n",
    "                          [\"ne contient pas\", \"ne mentionne pas\", \"ne traite pas\",\n",
    "                           \"does not contain\", \"does not mention\", \"does not address\"]):\n",
    "                    response += disclaimer\n",
    "\n",
    "        return response\n",
    "\n",
    "# Define API models\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str = Field(..., description=\"The legal question to ask\")\n",
    "    field: str = Field(..., description=\"The legal domain to query (criminal or constitution)\")\n",
    "    language: str = Field(\"fr\", description=\"Response language (fr or en)\")\n",
    "    top_k: int = Field(3, description=\"Number of documents to retrieve\", ge=1, le=5)\n",
    "    max_tokens: int = Field(150, description=\"Maximum number of tokens to generate\", ge=50, le=500)\n",
    "    temperature: float = Field(0.7, description=\"Temperature for text generation\", ge=0.1, le=1.0)\n",
    "\n",
    "class RetrievedDocument(BaseModel):\n",
    "    text: str\n",
    "    score: float\n",
    "    metadata: Dict\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    answer: str\n",
    "    retrieved_documents: List[RetrievedDocument]\n",
    "    query_time_ms: float\n",
    "    cache_hit: bool = False\n",
    "\n",
    "# Initialize components\n",
    "model_manager = ModelManager(MODEL_CONFIG)\n",
    "document_manager = DocumentManager(DOC_CONFIG)\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    try:\n",
    "        # Load models\n",
    "        await model_manager.load_models()\n",
    "        \n",
    "        # Try to load documents\n",
    "        docs_loaded = await document_manager.load_documents()\n",
    "        \n",
    "        if not docs_loaded:\n",
    "            logger.warning(\"Failed to load documents during startup. API will attempt to load on first request.\")\n",
    "        \n",
    "        logger.info(\"API started successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Startup error: {str(e)}\")\n",
    "        # Don't raise the exception - let the API start anyway\n",
    "        # We'll handle document loading errors on a per-request basis\n",
    "\n",
    "@app.post(\"/query\", response_model=QueryResponse)\n",
    "async def query_legal_assistant(request: QueryRequest, background_tasks: BackgroundTasks):\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Create cache key\n",
    "        cache_key = f\"{request.query}_{request.field}_{request.language}_{request.top_k}_{request.max_tokens}_{request.temperature}\"\n",
    "\n",
    "        # Check cache for identical query\n",
    "        if cache_key in cache.get(\"responses\", {}):\n",
    "            result = cache[\"responses\"][cache_key].copy()\n",
    "            result[\"cache_hit\"] = True\n",
    "            result[\"query_time_ms\"] = 0.0\n",
    "            logger.info(f\"Cache hit for query: {request.query[:30]}...\")\n",
    "            return result\n",
    "\n",
    "        # Validate field\n",
    "        if request.field not in DOC_CONFIG:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Invalid field. Choose from: {', '.join(DOC_CONFIG.keys())}\"\n",
    "            )\n",
    "\n",
    "        # Ensure documents are loaded\n",
    "        if not cache.get(\"texts\") or request.field not in cache.get(\"texts\", {}):\n",
    "            logger.info(\"Documents not loaded, attempting to load now\")\n",
    "            await document_manager.load_documents(force_reload=True)\n",
    "            \n",
    "            # Check again after loading attempt\n",
    "            if not cache.get(\"texts\") or request.field not in cache.get(\"texts\", {}):\n",
    "                raise HTTPException(\n",
    "                    status_code=500,\n",
    "                    detail=f\"Could not load documents for field: {request.field}. Please check file paths and try again.\"\n",
    "                )\n",
    "\n",
    "        # Initialize query processor\n",
    "        query_processor = QueryProcessor(model_manager)\n",
    "\n",
    "        # Process query to extract key information\n",
    "        query_info = await query_processor.preprocess_query(request.query)\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = await query_processor.search(\n",
    "            query=request.query,\n",
    "            field=request.field,\n",
    "            top_k=request.top_k\n",
    "        )\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            no_info_message = (\n",
    "                \"Je n'ai pas trouvé d'informations pertinentes dans les documents juridiques disponibles.\"\n",
    "                if request.language == \"fr\" else\n",
    "                \"I could not find relevant information in the available legal documents.\"\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"answer\": no_info_message,\n",
    "                \"retrieved_documents\": [],\n",
    "                \"query_time_ms\": (time.time() - start_time) * 1000,\n",
    "                \"cache_hit\": False\n",
    "            }\n",
    "\n",
    "            # Cache the response\n",
    "            cache.setdefault(\"responses\", {})\n",
    "            cache[\"responses\"][cache_key] = result\n",
    "\n",
    "            return result\n",
    "\n",
    "        # Create prompt\n",
    "        prompt = await query_processor.create_prompt(\n",
    "            query=request.query,\n",
    "            retrieved_documents=retrieved_docs,\n",
    "            query_info=query_info,\n",
    "            language=request.language\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        raw_response = model_manager.generate_response(\n",
    "            prompt=prompt,\n",
    "            max_tokens=request.max_tokens,\n",
    "            temperature=request.temperature\n",
    "        )\n",
    "\n",
    "        # Post-process response\n",
    "        processed_response = await query_processor.post_process_response(\n",
    "            response=raw_response,\n",
    "            query_info=query_info,\n",
    "            retrieved_docs=retrieved_docs,\n",
    "            language=request.language\n",
    "        )\n",
    "\n",
    "        # Calculate query time\n",
    "        query_time_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "        result = {\n",
    "            \"answer\": processed_response,\n",
    "            \"retrieved_documents\": retrieved_docs,\n",
    "            \"query_time_ms\": query_time_ms,\n",
    "            \"cache_hit\": False\n",
    "        }\n",
    "\n",
    "        # Cache the response\n",
    "        cache.setdefault(\"responses\", {})\n",
    "        cache[\"responses\"][cache_key] = result\n",
    "\n",
    "        # Schedule cache cleanup in background if it's getting too large\n",
    "        if len(cache[\"responses\"]) > 1000:\n",
    "            background_tasks.add_task(clean_response_cache)\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Query error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "async def clean_response_cache():\n",
    "    \"\"\"Clean up the response cache if it gets too large\"\"\"\n",
    "    global cache\n",
    "    if len(cache.get(\"responses\", {})) > 1000:\n",
    "        # Keep only the most recent 500 responses\n",
    "        sorted_keys = sorted(\n",
    "            cache[\"responses\"].keys(),\n",
    "            key=lambda k: cache[\"responses\"][k].get(\"timestamp\", 0),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        new_cache = {}\n",
    "        for key in sorted_keys[:500]:\n",
    "            new_cache[key] = cache[\"responses\"][key]\n",
    "\n",
    "        cache[\"responses\"] = new_cache\n",
    "        logger.info(f\"Cleaned response cache, now contains {len(cache['responses'])} items\")\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/reload\")\n",
    "async def reload_documents():\n",
    "    try:\n",
    "        await document_manager.load_documents(force_reload=True)\n",
    "        # Clear the response cache when documents are reloaded\n",
    "        cache[\"responses\"] = {}\n",
    "        return {\"status\": \"success\", \"message\": \"Documents reloaded successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Reload error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/stats\")\n",
    "async def get_stats():\n",
    "    \"\"\"Get API usage statistics\"\"\"\n",
    "    return {\n",
    "        \"total_queries\": len(cache.get(\"responses\", {})),\n",
    "        \"documents_loaded\": {\n",
    "            field: len(texts) for field, texts in cache.get(\"texts\", {}).items()\n",
    "        },\n",
    "        \"articles_extracted\": {\n",
    "            field: len(articles) for field, articles in cache.get(\"articles\", {}).items()\n",
    "        },\n",
    "        \"cache_hit_ratio\": sum(1 for r in cache.get(\"responses\", {}).values() if r.get(\"cache_hit\", False)) /\n",
    "                          max(1, len(cache.get(\"responses\", {})))\n",
    "    }\n",
    "\n",
    "# Example query for the specific article mentioned in your code\n",
    "example_query = {\n",
    "  \"query\": \"Quels sont les principes relatifs à l'imprescriptibilité selon l'article 21 de la Constitution tunisienne ?\",\n",
    "  \"field\": \"constitution\",\n",
    "  \"language\": \"fr\"\n",
    "}\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Now your uvicorn.run() should work\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tun_law_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
