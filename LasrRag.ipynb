{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ddd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb langchain pdfplumber sentence-transformers requests python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573677fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall transformers tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8f8a23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding models...\n",
      "Embedding models loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\", \"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\")\n",
    "\n",
    "# Initialize embedding models - only load what we need\n",
    "print(\"Loading embedding models...\")\n",
    "multilingual_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "print(\"Embedding models loaded.\")\n",
    "\n",
    "# Document processing functions\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF using PyMuPDF (fitz)\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    \"\"\"Load documents from a folder\"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Warning: Folder {folder_path} does not exist!\")\n",
    "        return docs\n",
    "    \n",
    "    print(f\"Looking for PDF files in {folder_path}...\")\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if filename.endswith(\".pdf\"):\n",
    "            print(f\"Processing PDF: {filename}\")\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            if text:\n",
    "                docs.append(Document(page_content=text, metadata={\"source\": filename}))\n",
    "        \n",
    "    print(f\"Loaded {len(docs)} documents\")\n",
    "    return docs\n",
    "\n",
    "def split_documents(documents, chunk_size=500, chunk_overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    split_docs = []\n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "        for chunk in chunks:\n",
    "            split_docs.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "    return split_docs\n",
    "\n",
    "# Build vector storage with hybrid retrieval\n",
    "import os\n",
    "\n",
    "class HybridRetriever:\n",
    "    def __init__(self, documents=None, use_faiss=True, faiss_index_path=\"faiss_index.bin\", chroma_path=\"chroma_store\"):\n",
    "        self.faiss_index_path = faiss_index_path\n",
    "        self.chroma_path = chroma_path\n",
    "\n",
    "        if documents:\n",
    "            self.documents = documents\n",
    "            self.document_texts = [doc.page_content for doc in documents]\n",
    "            self.document_sources = [doc.metadata for doc in documents]\n",
    "\n",
    "            print(\"Building BM25 index...\")\n",
    "            self.tokenized_corpus = [text.lower().split() for text in self.document_texts]\n",
    "            self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "\n",
    "            print(\"Encoding document embeddings...\")\n",
    "            self.embeddings = multilingual_model.encode(self.document_texts, convert_to_numpy=True)\n",
    "\n",
    "            if use_faiss:\n",
    "                self._build_faiss_index()\n",
    "\n",
    "            print(\"Building Chroma vectorstore...\")\n",
    "            self._build_chroma_vectorstore()\n",
    "        else:\n",
    "            print(\"Loading existing indices...\")\n",
    "            self.documents = []\n",
    "            self.document_texts = []\n",
    "            self.document_sources = []\n",
    "            self.bm25 = None\n",
    "            self.embeddings = None\n",
    "            self.index = None\n",
    "            self.chroma = None\n",
    "\n",
    "            if os.path.exists(self.faiss_index_path):\n",
    "                self._load_faiss_index()\n",
    "\n",
    "            if os.path.exists(self.chroma_path):\n",
    "                self._load_chroma_vectorstore()\n",
    "\n",
    "    def _build_faiss_index(self):\n",
    "        print(\"Building FAISS index...\")\n",
    "        d = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(d)\n",
    "        self.index.add(self.embeddings)\n",
    "        faiss.write_index(self.index, self.faiss_index_path)\n",
    "        print(f\"FAISS index saved to {self.faiss_index_path}\")\n",
    "\n",
    "    def _load_faiss_index(self):\n",
    "        print(f\"Loading FAISS index from {self.faiss_index_path}...\")\n",
    "        self.index = faiss.read_index(self.faiss_index_path)\n",
    "        print(\"FAISS index loaded.\")\n",
    "\n",
    "    def _build_chroma_vectorstore(self):\n",
    "        embedding_function = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        )\n",
    "        self.chroma = Chroma.from_documents(documents=self.documents, embedding=embedding_function, persist_directory=self.chroma_path)\n",
    "        self.chroma.persist()\n",
    "        print(f\"Chroma vectorstore saved to {self.chroma_path}\")\n",
    "\n",
    "    def _load_chroma_vectorstore(self):\n",
    "        print(f\"Loading Chroma vectorstore from {self.chroma_path}...\")\n",
    "        embedding_function = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        )\n",
    "        self.chroma = Chroma(persist_directory=self.chroma_path, embedding=embedding_function)\n",
    "        print(\"Chroma vectorstore loaded.\")\n",
    "\n",
    "    def search(self, query, top_k=5, hybrid_weight=0.7):\n",
    "        if not self.documents:\n",
    "            return []\n",
    "\n",
    "        query_embedding = multilingual_model.encode([query])[0]\n",
    "\n",
    "        # BM25 retrieval\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        bm25_scores = bm25_scores / (np.max(bm25_scores) + 1e-8)  # Normalize\n",
    "\n",
    "        # Semantic retrieval\n",
    "        semantic_scores = cosine_similarity([query_embedding], self.embeddings)[0]\n",
    "\n",
    "        # Combine scores\n",
    "        combined_scores = hybrid_weight * semantic_scores + (1 - hybrid_weight) * bm25_scores\n",
    "\n",
    "        # Get top-k indices and scores\n",
    "        top_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                \"content\": self.document_texts[idx],\n",
    "                \"metadata\": self.document_sources[idx],\n",
    "                \"score\": float(combined_scores[idx])\n",
    "            })\n",
    "\n",
    "        # Additional retrieval from Chroma\n",
    "        chroma_results = self.chroma.similarity_search_with_score(query, k=top_k)\n",
    "        chroma_docs = [{\"content\": doc.page_content, \"metadata\": doc.metadata, \"score\": score}\n",
    "                       for doc, score in chroma_results]\n",
    "\n",
    "        # Merge results\n",
    "        all_contents = set([r[\"content\"] for r in results])\n",
    "        for doc in chroma_docs:\n",
    "            if doc[\"content\"] not in all_contents:\n",
    "                results.append(doc)\n",
    "                all_contents.add(doc[\"content\"])\n",
    "\n",
    "        results = results[:top_k]\n",
    "        return results\n",
    "\n",
    "def query_openrouter_llm(context, question, language=\"fr\"):\n",
    "    \"\"\"Query LLM through OpenRouter API\"\"\"\n",
    "    if language == \"fr\":\n",
    "        prompt = f\"\"\"\n",
    "        En vous basant sur les extraits juridiques suivants, répondez à la question en français.\n",
    "        Si les extraits ne contiennent pas d'information pertinente pour répondre, indiquez-le clairement.\n",
    "        \n",
    "        Extraits juridiques:\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        \n",
    "        model = \"deepseek/deepseek-chat-v3-0324\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "        Based on the following legal extracts, answer the question in English.\n",
    "        If the extracts don't contain relevant information to answer, clearly state this.\n",
    "        \n",
    "        Legal extracts:\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        \n",
    "        model = \"anthropic/claude-3-sonnet-20240229\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", \n",
    "                          headers=headers, json=data)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API Error: {response.status_code} - {response.text}\")\n",
    "    \n",
    "    res_json = response.json()\n",
    "    if \"choices\" not in res_json:\n",
    "        raise Exception(f\"Unexpected API response: {res_json}\")\n",
    "    \n",
    "    return res_json[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "def query_rag(question, retriever, top_k=5, language=\"fr\"):\n",
    "    \"\"\"Run RAG pipeline with improved retrieval\"\"\"\n",
    "    # Get document chunks relevant to the question\n",
    "    retrieved_docs = retriever.search(question, top_k=top_k)\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        if language == \"fr\":\n",
    "            return \"Aucun document pertinent trouvé pour cette question.\", []\n",
    "        else:\n",
    "            return \"No relevant documents found for this question.\", []\n",
    "    \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"content\"][:1500] for doc in retrieved_docs])\n",
    "    print(context)\n",
    "    try:\n",
    "        # Query LLM with context and question\n",
    "        answer = query_openrouter_llm(context, question, language)\n",
    "        \n",
    "        # Extract sources for citation\n",
    "        sources = [doc[\"metadata\"] for doc in retrieved_docs]\n",
    "        \n",
    "        return answer, sources\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM: {e}\")\n",
    "        if language == \"fr\":\n",
    "            return f\"Erreur lors de la génération de la réponse: {str(e)}\", []\n",
    "        else:\n",
    "            return f\"Error generating response: {str(e)}\", []\n",
    "\n",
    "# This class helps integrate the RAG system into a Jupyter notebook\n",
    "class LegalRAGSystem:\n",
    "    def __init__(self, documents_folder=\"documents\"):\n",
    "        self.documents_folder = documents_folder\n",
    "        self.retriever = None\n",
    "        self.loaded = False\n",
    "    \n",
    "    def load_documents(self, folder_path=None):\n",
    "        \"\"\"Load documents from specified folder or use default\"\"\"\n",
    "        if folder_path:\n",
    "            self.documents_folder = folder_path\n",
    "        \n",
    "        print(f\"Loading documents from {self.documents_folder}\")\n",
    "        documents = load_documents(self.documents_folder)\n",
    "        \n",
    "        if documents:\n",
    "            chunked_docs = split_documents(documents)\n",
    "            self.retriever = HybridRetriever(chunked_docs)\n",
    "            self.loaded = True\n",
    "            return f\"Loaded {len(documents)} documents, created {len(chunked_docs)} chunks\"\n",
    "        else:\n",
    "            print(\"No documents found. Please check the path and try again.\")\n",
    "            return \"No documents loaded\"\n",
    "    \n",
    "    def query(self, question, language=\"fr\", top_k=5):\n",
    "        \"\"\"Query the system with a question\"\"\"\n",
    "        if not self.loaded:\n",
    "            return \"System not loaded. Please load documents first.\"\n",
    "        \n",
    "        answer, sources = query_rag(question, self.retriever, top_k, language)\n",
    "        \n",
    "        # Format sources for display\n",
    "        source_info = []\n",
    "        for src in sources:\n",
    "            source_name = src.get('source', 'Unknown')\n",
    "            chunk_num = src.get('chunk', 'Unknown')\n",
    "            total_chunks = src.get('chunk_of', 'Unknown')\n",
    "            source_info.append(f\"{source_name} (chunk {chunk_num}/{total_chunks})\")\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": source_info\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a918437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BM25 index...\n",
      "Encoding document embeddings...\n",
      "Building FAISS index...\n",
      "FAISS index saved to faiss_index.bin\n",
      "Building Chroma vectorstore...\n",
      "Chroma vectorstore saved to chroma_store\n",
      "constituante. \n",
      "Demeurent en vigueur, jusqu’à l’élection du Président de la \n",
      "République conformément aux dispositions de l’article 74 et suivants \n",
      "de la Constitution, les dispositions des articles 7, 9 à 14 et de l’article \n",
      "26 de l’Organisation provisoire des pouvoirs publics.  \n",
      "Demeurent en vigueur, jusqu’à ce que le premier Gouvernement \n",
      "obtienne la confiance de l’Assemblée des représentants du peuple, les \n",
      "articles 17 à 20 de l’Organisation\n",
      ")\n",
      "1\n",
      "(  provisoire des pouvoirs publics.\n",
      "\n",
      "---\n",
      "\n",
      "9\n",
      "1 à 20\n",
      "Chapitre I : Des principes généraux……..…..…...\n",
      "13\n",
      "21 à 49\n",
      "Chapitre II : Des droits et libertés……….......…...\n",
      "19\n",
      "50 à 70\n",
      "Chapitre III : Du pouvoir législatif……….……... \n",
      "27\n",
      "71 à 101\n",
      "Chapitre IV : Du pouvoir exécutif ………..……...\n",
      "27\n",
      "72 à 88\n",
      "Section I : Du Président de la République………... \n",
      "34\n",
      "89 à 101\n",
      "Section II : Du Gouvernement……….......…...…... \n",
      "41\n",
      "102 à 124\n",
      "Chapitre V : Du pouvoir juridictionnel ………... \n",
      "41\n",
      "106 à 117\n",
      "Section I : De la justice judiciaire, administrative et\n",
      "\n",
      "---\n",
      "\n",
      "Imprimerie Officielle de la République Tunisienne\n",
      "Constitution\n",
      "de la République\n",
      "Tunisienne\n",
      "61\n",
      "TABLE DES MATIERES  \n",
      "Pages\n",
      "Articles \n",
      "Matière \n",
      "5\n",
      "1 et 2\n",
      "Décision du président de l’assemblée nationale \n",
      "constituante du 30 rabiaa I 1435-31 janvier 2014, \n",
      "ordonnant la publication de la constitution de la \n",
      "République Tunisienne ………………………..…\n",
      "7\n",
      "-\n",
      "Préambule …………...…………………………...\n",
      "9\n",
      "1 à 149\n",
      "Texte de la Constitution de la République Tunisienne.\n",
      "9\n",
      "1 à 20\n",
      "Chapitre I : Des principes généraux……..…..…...\n",
      "13\n",
      "\n",
      "---\n",
      "\n",
      "civiles dans les conditions fixées par la loi. \n",
      "Article 19 : \n",
      "La sûreté nationale est républicaine; ses forces sont chargées de \n",
      "maintenir la sécurité et l’ordre public, de protéger les individus, les \n",
      "institutions et les biens, et d’exécuter la loi dans le respect des libertés \n",
      "et de la neutralité totale. \n",
      "Article 20 : \n",
      "Les conventions approuvées par le Parlement et ratifiées sont \n",
      "supérieures aux lois et inférieures à la Constitution.\n",
      "\n",
      "---\n",
      "\n",
      "articles 17 à 20 de l’Organisation\n",
      ")\n",
      "1\n",
      "(  provisoire des pouvoirs publics. \n",
      "Jusqu’à l’élection de l’Assemblée des représentants du peuple, \n",
      "l’Assemblée nationale constituante continue à exercer ses fonctions \n",
      "législatives et de contrôle, ainsi que ses attributions électorales \n",
      "prévues par la loi constituante relative à l’organisation provisoire des \n",
      "pouvoirs publics ou les lois en vigueur. \n",
      "2. Les dispositions ci-après entrent en vigueur ainsi qu’il suit :\n",
      "Answer:\n",
      "L'article 20 est cité dans les extraits juridiques fournis. Voici son contenu :  \n",
      "\n",
      "**Article 20 :**  \n",
      "*\"Les conventions approuvées par le Parlement et ratifiées sont supérieures aux lois et inférieures à la Constitution.\"*  \n",
      "\n",
      "Cet article fait partie de l'Organisation provisoire des pouvoirs publics et reste en vigueur jusqu'à ce que le premier Gouvernement obtienne la confiance de l'Assemblée des représentants du peuple, conformément aux extraits.\n",
      "\n",
      "Sources:\n",
      "- Constitution_fr.pdf (chunk Unknown/Unknown)\n",
      "- Constitution_fr.pdf (chunk Unknown/Unknown)\n",
      "- Constitution_fr.pdf (chunk Unknown/Unknown)\n",
      "- Constitution_fr.pdf (chunk Unknown/Unknown)\n",
      "- Constitution_fr.pdf (chunk Unknown/Unknown)\n"
     ]
    }
   ],
   "source": [
    "# First execute the code above\n",
    "\n",
    "# Initialize the system\n",
    "rag_system = LegalRAGSystem()\n",
    "\n",
    "# Load documents from a specific folder\n",
    "\n",
    "document = Document(page_content=extract_text_from_pdf(r\"C:\\Users\\mouni\\tun_law_project\\documents\\Constitution_fr.pdf\"), metadata={\"source\": \"Constitution_fr.pdf\"})\n",
    "chunked_docs = split_documents([document])\n",
    "rag_system.retriever = HybridRetriever(chunked_docs)\n",
    "rag_system.loaded = True\n",
    "\n",
    "# Query the system\n",
    "result = rag_system.query(\"citez l'article 20\")\n",
    "\n",
    "# Display the result\n",
    "print(\"Answer:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSources:\")\n",
    "for source in result[\"sources\"]:\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47700372",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 427)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:427\u001b[1;36m\u001b[0m\n\u001b[1;33m    context = \"\\n\\n---\\n\\n\".join(context_parts)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b082523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
